import streamlit as st
from anthropic import Anthropic

max_input_token=40000

client = Anthropic(api_key=st.secrets['ANTHROPIC_API_KEY'])

def claude_stream_generator(response_stream):
    """Claude APIì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ í…ìŠ¤íŠ¸ ì œë„ˆë ˆì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    for chunk in response_stream:
        if hasattr(chunk, 'type'):
            # content_block_delta ì´ë²¤íŠ¸ ì²˜ë¦¬
            if chunk.type == 'content_block_delta' and hasattr(chunk, 'delta') and hasattr(chunk.delta, 'text'):
                yield chunk.delta.text
            # content_block_start ì´ë²¤íŠ¸ ì²˜ë¦¬
            elif chunk.type == 'content_block_start' and hasattr(chunk, 'content_block') and hasattr(chunk.content_block, 'text'):
                yield chunk.content_block.text

def get_preview_with_claude(messages):
    user_messages = [m['content'] for m in messages if m.get('role') == 'user']
    message_in_string = "\n".join(f"- {msg}" for msg in user_messages[:5]) 

    prompt = f"""ë‹¤ìŒ ëŒ€í™”ì˜ ì œëª©ì„ í•œê¸€ 10ì ì´ë‚´ ë˜ëŠ” ì˜ì–´ 20ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì œëª©ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. 
               {message_in_string}
              ì œëª©:"""
    response = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=64,
        temperature=0.2,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.content[0].text.strip().split('\n')[0]

#ì…ë ¥ í† í° ì¹´ìš´íŒ…
def count_token(model, system, messages):
    response = client.messages.count_tokens(
        model=model,
        system=system,
        messages=messages,
    )
    return int(dict(response)['input_tokens'])


def truncate_messages(messages, system_prompt, max_tokens=max_input_token):
    """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì‚°ì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€í™” ê¸¸ì´ ì œí•œ"""
    if len(messages) == 0:
        return messages

    # í˜„ì¬ ì „ì²´ í† í° ìˆ˜ ê³„ì‚°
    current_tokens = count_token("claude-sonnet-4-20250514", system_prompt, messages)

    # í† í° ìˆ˜ê°€ ì œí•œ ì´í•˜ë©´ ì „ì²´ ë°˜í™˜
    if current_tokens <= max_tokens:
        return messages, current_tokens

    # í† í° ìˆ˜ê°€ ì´ˆê³¼í•˜ë©´ ë¹„ë¡€ì ìœ¼ë¡œ ëŒ€í™” ìˆ˜ ì¤„ì´ê¸°
    total_conversations = len(messages) // 2  # user+assistant ìŒì˜ ê°œìˆ˜
    if total_conversations == 0:
        return messages, current_tokens

    # ìœ ì§€í•  ëŒ€í™” ìˆ˜ ê³„ì‚° (ìµœì†Œ 1ê°œëŠ” ë³´ì¥)
    keep_conversations = max(1, int(total_conversations * (max_tokens / current_tokens)))

    # ìµœê·¼ Nê°œ ëŒ€í™”ë§Œ ìœ ì§€ (user+assistant ìŒ ë‹¨ìœ„)
    keep_messages_count = keep_conversations * 2
    truncated_messages = messages[-keep_messages_count:]
    return truncated_messages, int(current_tokens * (max_tokens / current_tokens))
        

def generate_claude_response(model, temperature, system_prompt):
    messages = [
        {"role": m["role"], "content": m["content"]}
        for m in st.session_state.messages
    ]
    truncated_messages, num_input_tokens = truncate_messages(messages, system_prompt, max_tokens=max_input_token)
    st.session_state.num_input_tokens = num_input_tokens
    
    try:
        # API í˜¸ì¶œ
        with st.spinner("Claudeê°€ ì‘ë‹µ ì¤‘..."):
            # ìƒˆë¡œìš´ chat_message ì»¨í…Œì´ë„ˆ ìƒì„±
            with st.chat_message("assistant"):
                # ì´ˆê¸° í…ìŠ¤íŠ¸ë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
                response_placeholder = st.empty()
                response_placeholder.markdown("")
                
                response = client.messages.create(
                    model=model,
                    messages=truncated_messages,
                    temperature=temperature,
                    max_tokens=64000,
                    system=system_prompt,
                    stream=True
                )
                
                # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
                full_response = ""
                for text in claude_stream_generator(response):
                    full_response += text
                    # ì‘ë‹µ ì—…ë°ì´íŠ¸
                    response_placeholder.markdown(full_response)
            
                # ë©”ì‹œì§€ ê¸°ë¡ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                
        # ì‘ë‹µ ìƒì„± ì™„ë£Œ
        st.session_state.generating_response = False

    except Exception as e:
        # ì¬ì‹œë„ ë¶ˆê°€ëŠ¥í•œ ëª…í™•í•œ ì˜¤ë¥˜ë“¤
        error_str = str(e)
        if ('overloaded_error' in error_str or 
            'rate_limit' in error_str or 
            'authentication' in error_str or
            'permission' in error_str):
            if 'overloaded_error' in error_str:
                st.error("ì´ëŸ°, Anthropic ì„œë²„ê°€ ì£½ì–´ìˆë„¤ìš”ğŸ˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì£¼ì„¸ìš”")
            else:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.session_state.generating_response = False
        else:
            # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì˜¤ë¥˜ - í”Œë˜ê·¸ ìœ ì§€í•˜ê³  ì¡°ìš©íˆ ì‹¤íŒ¨
            # ìƒìœ„ì—ì„œ ì¬ê²€ì¦ ë¡œì§ì´ ì¬ì‹œë„í•  ê²ƒì„
            pass
        
